{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.keras import datasets   # 我们使用这个函数来下载数据\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet, testSet = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CifarModel(object):    \n",
    "    def __init__(self, lr, batch_size, iter_num):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.iter_num = iter_num\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.y = tf.placeholder(tf.int32, [None, 10])        \n",
    "        self.dropRate = tf.placeholder(tf.float32)                    \n",
    " \n",
    "        conv1 = tf.layers.conv2d(self.X, 32, 5, padding='same', activation=tf.nn.relu,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.0001, seed=0),\n",
    "                                     bias_initializer=tf.constant_initializer(0.001))        \n",
    "        pool1 = tf.layers.max_pooling2d(conv1 , 3, 2, padding='same')           \n",
    "        conv2 = tf.layers.conv2d(pool1, 32, 5, padding='same', activation=tf.nn.relu,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "                                     bias_initializer=tf.constant_initializer(0.001))\n",
    "        pool2 = tf.layers.average_pooling2d(conv2, 3,2, padding='same')          \n",
    "        conv3 = tf.layers.conv2d(pool2, 64, 5, padding='same', activation=tf.nn.relu,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "                                     bias_initializer=tf.constant_initializer(0.001))\n",
    "        pool3 = tf.layers.average_pooling2d(conv3, 3,2, padding='same')  \n",
    "\n",
    "        flatten = tf.reshape(pool3 , [self.batch_size, 4*4*64])\n",
    "        dense1 = tf.layers.dense(flatten, 64,  activation=tf.nn.relu, use_bias=True,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "                                 bias_initializer=tf.constant_initializer(0.001))      \n",
    "        dense1 = tf.nn.dropout(dense1, self.dropRate)\n",
    "        dense2 = tf.layers.dense(dense1, 10, use_bias=True,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.01, seed=0),\n",
    "                                 bias_initializer=tf.constant_initializer(0.1))  \n",
    "        \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.y, logits=dense2)\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss )        \n",
    "\n",
    "        # 用于模型训练\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.y, axis=1), tf.argmax(dense2, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        # 用于保存训练好的模型\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        summary_loss = tf.summary.scalar('loss', self.loss)\n",
    "        summary_accuracy = tf.summary.scalar('accuracy', self.accuracy)\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "    def get_next_train_batch(self):        \n",
    "        m = 0\n",
    "        while True:\n",
    "            batch_x = trainSet[0][m:m+self.batch_size]\n",
    "            batch_y = trainSet[1][m:m+self.batch_size]\n",
    "\n",
    "            m += self.batch_size\n",
    "            if m+self.batch_size > 50000:\n",
    "                m=0\n",
    "            yield batch_x, (np.arange(10) == batch_y[:, None]).astype(int).reshape(self.batch_size,10)\n",
    "        \n",
    "            \n",
    "    def get_next_test_batch(self):        \n",
    "        n = 0\n",
    "        while True:\n",
    "            batch_x = testSet[0][n:n+self.batch_size]\n",
    "            batch_y = testSet[1][n:n+self.batch_size]\n",
    "\n",
    "            n += self.batch_size\n",
    "            if n+self.batch_size > 10000:\n",
    "                n=0\n",
    "            yield batch_x, (np.arange(10) == batch_y[:, None]).astype(int).reshape(self.batch_size,10)\n",
    "            \n",
    "    def train(self):\n",
    "        \n",
    "        with tf.Session() as sess:            #  打开一个会话。可以想象成浏览器打开一个标签页一样，直观地理解一下\n",
    "            sess.run(tf.global_variables_initializer())  # 先初始化所有变量。\n",
    "            generator = self.get_next_train_batch()  # 读取一批数据\n",
    "            genetator_test = self.get_next_test_batch()\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter('log/train_base', sess.graph)\n",
    "            summary_writer_test = tf.summary.FileWriter('log/test_base')\n",
    "            \n",
    "            for i in range(self.iter_num):\n",
    "                batch_x, batch_y = generator.next()                  \n",
    "                loss, _= sess.run([self.loss, self.train_step], feed_dict={self.X: batch_x, self.y: batch_y, self.dropRate:0.5})   # 每调用一次sess.run，就像拧开水管一样，所有self.loss和self.train_step涉及到的运算都会被调用一次。\n",
    "                \n",
    "                if i%1000 == 0:  \n",
    "                    batch_x, batch_y = generator.next()             \n",
    "                    train_accuracy, summary_str = sess.run([self.accuracy, self.merged_summary_op], feed_dict={self.X: batch_x, self.y: batch_y, self.dropRate:1.})  # 把训练集数据装填进去\n",
    "                    summary_writer.add_summary(summary_str, i)                    \n",
    "                    test_x, test_y = genetator_test.next()\n",
    "                    test_accuracy, summary_str = sess.run([self.accuracy, self.merged_summary_op], feed_dict={self.X: test_x, self.y: test_y, self.dropRate:1.})   # 把测试集数据装填进去\n",
    "                    summary_writer_test.add_summary(summary_str, i)\n",
    "                    print ('iter\\t%i\\tloss\\t%f\\ttrain_accuracy\\t%f\\ttest_accuracy\\t%f' % (i,loss,train_accuracy,test_accuracy))\n",
    "            self.saver.save(sess, 'model/cifarModel') # 保存模型\n",
    "            summary_writer.flush()\n",
    "            summary_writer_test.flush()\n",
    "            \n",
    "    def test(self):\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, 'model/cifarModel')\n",
    "            genetator_test = self.get_next_test_batch()\n",
    "            \n",
    "            Accuracy = []\n",
    "            for i in range(int(10000 / self.batch_size)):\n",
    "                test_x, test_y = genetator_test.next()\n",
    "                test_accuracy = sess.run(self.accuracy, feed_dict={self.X: test_x, self.y: test_y,self.dropRate:1.0})\n",
    "                Accuracy.append(test_accuracy)\n",
    "            print( '==' * 15)\n",
    "            print( 'Test Accuracy: ', np.mean(np.array(Accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e2d4b3575c5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCifarModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 学习率为0.01 每一批50张图 训练3000次\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-07bc2e813a92>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropRate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 每调用一次sess.run，就像拧开水管一样，所有self.loss和self.train_step涉及到的运算都会被调用一次。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# 模型训练与评估\n",
    "\n",
    "model = CifarModel(0.01, 50, 3000)  # 学习率为0.01 每一批50张图 训练3000次\n",
    "model.train()\n",
    "model.test_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
