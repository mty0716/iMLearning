{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "train_file = \"./data/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = sys.getdefaultencoding()\n",
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r', encoding = enc) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            # comment the following line if you use nltk\n",
    "            en.append([\"BOS\"] + line[0].split() + [\"EOS\"])\n",
    "            # uncomment the following line if you installed nltk\n",
    "#             en.append([\"BOS\"] + nltk.word_tokenize(line[0]) + [\"EOS\"]) \n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "train_en, train_cn = load_data(train_file)\n",
    "num_train = len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', 'Anyone', 'can', 'do', 'that.', 'EOS'],\n",
       " ['BOS', 'How', 'about', 'another', 'piece', 'of', 'cake?', 'EOS'],\n",
       " ['BOS', 'She', 'married', 'him.', 'EOS'],\n",
       " ['BOS', 'I', \"don't\", 'like', 'learning', 'irregular', 'verbs.', 'EOS'],\n",
       " ['BOS', \"It's\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me.', 'EOS']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'],\n",
       " ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS'],\n",
       " ['BOS', '她', '嫁', '给', '了', '他', '。', 'EOS'],\n",
       " ['BOS', '我', '不', '喜', '欢', '学', '习', '不', '规', '则', '动', '词', '。', 'EOS'],\n",
       " ['BOS',\n",
       "  '這',\n",
       "  '對',\n",
       "  '我',\n",
       "  '來',\n",
       "  '說',\n",
       "  '是',\n",
       "  '個',\n",
       "  '全',\n",
       "  '新',\n",
       "  '的',\n",
       "  '球',\n",
       "  '類',\n",
       "  '遊',\n",
       "  '戲',\n",
       "  '。',\n",
       "  'EOS']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "model_dir = \"seq2seq\"\n",
    "make_dir(model_dir)\n",
    "\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 1\n",
    "    word_dict = {w[0]: index+1 for (index, w) in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = 0\n",
    "    return word_dict, total_words\n",
    "\n",
    "vocab_file = os.path.join(model_dir, \"vocab.pkl\")\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "    \n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = []\n",
    "    out_cn_sentences = []\n",
    "\n",
    "    for i in range(length):\n",
    "        en_seq = [en_dict[w] if w in en_dict else 0 for w in en_sentences[i]]\n",
    "        cn_seq = [cn_dict[w] if w in cn_dict else 0 for w in cn_sentences[i]]\n",
    "        out_en_sentences.append(en_seq)\n",
    "        out_cn_sentences.append(cn_seq)\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4612, 2], [1, 3454, 2], [1, 2688, 2], [1, 5436, 2], [1, 3454, 2]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7, 86, 440, 5, 3, 2],\n",
       " [1, 118, 1367, 220, 2],\n",
       " [1, 981, 2027, 7, 3, 2],\n",
       " [1, 238, 238, 220, 2],\n",
       " [1, 150, 189, 220, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "import numpy as np\n",
    "\n",
    "# get minibatches of \n",
    "def get_minibatches(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches\n",
    "\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, max_len)).astype('float32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        x_mask[idx, :lengths[idx]] = 1.0\n",
    "    return x, x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_mask = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_mask = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_mask, mb_y, mb_y_mask))\n",
    "    return all_ex\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers = 1):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(self.hidden_size)\n",
    "        \n",
    "    def __call__(self, inputs, seq_length, state=None):\n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        for i in range(self.num_layers):\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, sequence_length=seq_length, initial_state=state, dtype=tf.float32)\n",
    "        return out, state\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, embedding, hidden_size, num_layers=1, max_length=15):\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell = rnn.GRUCell(hidden_size)\n",
    "        self.linear = tf.Variable(tf.random_normal(shape=(self.hidden_size, cn_total_words))*0.1)\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs, state, encoder_state): # context vector\n",
    "        \n",
    "        out = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        out = tf.tile(tf.expand_dims(encoder_state, 1), (1, tf.shape(out)[1], 1))\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "#             state = tf.concat([state, encoder_state], 1)\n",
    "            out, state = tf.nn.dynamic_rnn(self.cell, out, initial_state=state, dtype=tf.float32)\n",
    "    \n",
    "        out = tf.tensordot(out, self.linear, axes=[[2], [0]])\n",
    "        return out, state\n",
    "\n",
    "class Seq2Seq:\n",
    "    def __init__(self, hidden_size, num_layers, embed_words_en, embed_words_cn):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = 15\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.name_scope(\"place_holder\"):\n",
    "                self.encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"encoder_inputs\")\n",
    "                self.encoder_length = tf.placeholder(shape=(None, ), dtype=tf.int64, name=\"encoder_length\")\n",
    "                self.decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_inputs\")\n",
    "                self.decoder_target = tf.placeholder(shape=(None, None), dtype=tf.int64, name=\"decoder_target\")\n",
    "                self.decoder_mask = tf.placeholder(shape=(None, None), dtype=tf.float32, name=\"decoder_mask\")\n",
    "\n",
    "            with tf.name_scope(\"embedding\"):\n",
    "                self.embedding_en = tf.get_variable(name=\"embedding_en\", dtype=tf.float32, shape=(en_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_en))\n",
    "                self.embedding_cn = tf.get_variable(name=\"embedding_cn\", dtype=tf.float32, shape=(cn_total_words, hidden_size),\n",
    "                                                    initializer=tf.constant_initializer(embed_words_cn))\n",
    "            with tf.name_scope(\"encoder-decoder\"):\n",
    "                self.encoder = Encoder(self.embedding_en, self.hidden_size, self.num_layers)\n",
    "                self.decoder = Decoder(self.embedding_cn + self.hidden_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-train\"):\n",
    "                encoder_outputs, encoder_state = self.encoder(self.encoder_inputs, self.encoder_length)\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = self.decoder_inputs\n",
    "\n",
    "                decoder_outputs, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "\n",
    "                # decoder_outputs.append(decoder_out)\n",
    "                decoder_outputs = tf.concat(decoder_outputs, 1)\n",
    "\n",
    "            with tf.name_scope(\"cost\"):\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_outputs, labels=self.decoder_target)\n",
    "\n",
    "                self.cost = tf.reduce_mean(loss * self.decoder_mask)\n",
    "\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n",
    "                optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
    "                self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "            with tf.variable_scope(\"seq2seq-generate\"):\n",
    "                self.generate_outputs = []\n",
    "                decoder_state = encoder_state\n",
    "                word_indices = tf.expand_dims(self.decoder_inputs[:, 0], 1)\n",
    "                for i in range(self.max_length):\n",
    "                    decoder_out, decoder_state = self.decoder(word_indices, decoder_state, encoder_state)\n",
    "                    softmax_out = tf.nn.softmax(decoder_out[:, 0, :])\n",
    "                    word_indices = tf.expand_dims(tf.cast(tf.argmax(softmax_out, -1), dtype=tf.int64), 1)\n",
    "                    self.generate_outputs.append(word_indices)\n",
    "                self.generate_outputs = tf.concat(self.generate_outputs, 0)\n",
    "            \n",
    "            \n",
    "    def train(self, sess, encoder_inputs, encoder_length, decoder_inputs, decoder_target, decoder_mask):\n",
    "        _, cost = sess.run([self.train_op, self.cost], feed_dict={\n",
    "            self.encoder_inputs: encoder_inputs, \n",
    "            self.encoder_length: encoder_length,\n",
    "            self.decoder_inputs: decoder_inputs,\n",
    "            self.decoder_target: decoder_target,\n",
    "            self.decoder_mask: decoder_mask\n",
    "        })\n",
    "        return cost\n",
    "    \n",
    "    def generate(self, sess, encoder_inputs, encoder_length):\n",
    "        decoder_inputs = np.asarray([[en_dict[\"BOS\"]]*15], dtype=\"int64\")\n",
    "        if encoder_inputs.ndim == 1:\n",
    "            encoder_inputs = encoder_inputs.reshape((1, -1))\n",
    "            encoder_length = encoder_length.reshape((-1))\n",
    "        generate = sess.run([self.generate_outputs],\n",
    "                           feed_dict={self.encoder_inputs: encoder_inputs,\n",
    "                                      self.decoder_inputs: decoder_inputs,\n",
    "                                      self.encoder_length: encoder_length})[0]\n",
    "        return generate\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.03904918821123932\n",
      "training loss: 0.028003083136342047\n",
      "training loss: 0.026187277968896278\n",
      "training loss: 0.025113068730703347\n",
      "training loss: 0.024215190554816815\n",
      "training loss: 0.023449937590038196\n",
      "training loss: 0.02278640773384453\n",
      "training loss: 0.022223041848815465\n",
      "training loss: 0.021739929662242623\n",
      "training loss: 0.021314881521041367\n",
      "training loss: 0.020936365310646896\n",
      "training loss: 0.020605757216302163\n",
      "training loss: 0.020285432766729512\n",
      "training loss: 0.019999991967296973\n",
      "training loss: 0.01975744895840564\n",
      "training loss: 0.01951054467605657\n",
      "training loss: 0.01928045381692881\n",
      "training loss: 0.019096784021425728\n",
      "training loss: 0.018875730666833398\n",
      "training loss: 0.018730067570260522\n",
      "training loss: 0.01853902256432202\n",
      "training loss: 0.018399991467774535\n",
      "training loss: 0.018231035254986427\n",
      "training loss: 0.018094596642633517\n",
      "training loss: 0.01792412493058789\n",
      "training loss: 0.017840988053791883\n",
      "training loss: 0.01767349811608552\n",
      "training loss: 0.0175832375470847\n",
      "training loss: 0.017464376140639892\n",
      "training loss: 0.017372803096473318\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "hidden_size = 50\n",
    "num_layers = 1\n",
    "emb_en = np.random.uniform(low=-0.1, high=0.1, size=(en_total_words, hidden_size))\n",
    "emb_cn = np.random.uniform(low=-0.1, high=0.1, size=(cn_total_words, hidden_size))\n",
    "model = Seq2Seq(hidden_size, num_layers, emb_en, emb_cn)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "epoch = 0\n",
    "n_epochs = 30\n",
    "# print(sess.run(model.decoder_state))\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    total_loss = 0 \n",
    "    total_num_ins = 0\n",
    "    for (encoder_inputs, encoder_length, mb_y, mb_y_mask) in train_data:\n",
    "        decoder_inputs = mb_y[:, :-1]\n",
    "        decoder_target = mb_y[:, 1:]\n",
    "#         print(encoder_length.sum(1).shape)\n",
    "        loss = model.train(sess, encoder_inputs, encoder_length.sum(1), decoder_inputs, decoder_target, mb_y_mask[:, :-1])\n",
    "        total_loss += loss\n",
    "        total_num_ins += mb_y.shape[0]\n",
    "    print(\"training loss: {}\".format(total_loss / total_num_ins))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS', 'Can', 'he', 'speak', 'English?', 'EOS']\n",
      "['他', '會', '講', '講', '會', '英', '英', '語', 'EOS', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = [inv_en_dict[c] for c in train_data[11][0][2]]\n",
    "print(encoder_inputs)\n",
    "encoder_inputs = [en_dict.get(e, 0) for e in encoder_inputs]\n",
    "encoder_inputs = np.asarray(encoder_inputs).reshape(1, -1)\n",
    "encoder_length = np.asarray([encoder_inputs.shape[1]]).reshape(-1)\n",
    "res = model.generate(sess, encoder_inputs, encoder_length).flatten()\n",
    "\n",
    "res = [inv_cn_dict[r] for r in res]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
